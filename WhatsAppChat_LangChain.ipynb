{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to install the following packages if you're a first time user:\n",
    "\n",
    "# !pip install langchain\n",
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Imports & Loading the OpenAI API Key from .env file. \n",
    "# Ensure that the API Key is properly configured. Please refer to the README.md for detailed instructions.\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the WhatsApp chat history and save it as a langchain document\n",
    "\n",
    "from langchain_community.document_loaders import WhatsAppChatLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WhatsAppChatLoader(\"data/chat_history.txt\")\n",
    "loader.load()\n",
    "\n",
    "docs = []\n",
    "docs.extend(loader.load())\n",
    "\n",
    "# Split the document into multiple chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings from the document chunks and generate a persistent vector database for future reuse. \n",
    "# This cell should be executed only once, ensuring that the chroma vector database remains available for subsequent use.\n",
    "# The number of vector database collections should match the number of splits above.\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "persist_directory = 'data/chroma_db/'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectordb.persist()\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Manoj Joshi IITB on 4/14/24, 01:11: Think of this as a massive class action settlement of\\n\\nManoj Joshi IITB on 4/14/24, 01:34: Introducing such a tax on the makers of \"Big Tech AI\" will also give them a chance to consider if at all they want to invest in AI after all, as I suspect some of them may decide not to invest in AI at the same pace, once they realize it\\'s not exclusively lucrative for them to operate under these new fiscal conditions. That will eliminate any false sense of a \"heated market\" and reduce the concern for a ballooning economy operating on hype rather than intrinsic value. One example of that is about the impact of GDPR on American companies who have sometimes made a conscious choice about not investing in reckless proliferation of software/apps in EU with little regard for data privacy and ethics. The net result is that the EU was compensated due to enforcement of GDPR on American big tech.\\n\\nManoj Joshi IITB on 4/14/24, 01:53: My points may make it seem like I am against big tech. But strangely I am not. In fact I would like big tech AI to succeed, but with balanced growth. And if they want to keep a larger portion of the wealth, this is fine too. We just need them to realize that they cannot outright replace all humans (or most humans) with bots and continue BAU (business as usual) as if it\\'s another day in decision making, without at the same time paying some form of a tax, which is the crux of my proposition.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask a question and get the relevant documents for it from the vector database\n",
    "\n",
    "question = \"What is discussed about AI Tax?\"\n",
    "docs = vectordb.similarity_search(question,k=5)\n",
    "len(docs)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the conversation, there is a proposal to introduce a special tax on \"Big Tech AI\" companies. The purpose of this tax would be to make these companies consider their investments in AI more carefully and to ensure that they contribute to society by paying taxes if they want to keep a larger portion of the wealth generated by AI. The idea is not to hinder AI progress but to promote balanced growth and ensure that companies do not replace human workers with AI without contributing to society through taxes. The discussion also touches on the need for a new form of economics if there is a full-blown AI takeover in the future.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use LangChain Retrieval to answer your questions based on the context in the vector database\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
